{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from standard_libs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ch07\"></a>\n",
    "# Chapter 7: Extracting Information from Text\n",
    "Questions to be answered:\n",
    "1. How can we build a system that extracts structured data, such as tables, from unstructured text?\n",
    "2. What are some robust methods for idenitfying the entities and relationships described in a text?\n",
    "3. Which corpora are appropriate for this work, and how do we use them for training and evaluating our models?\n",
    "\n",
    "\n",
    "* [Section 1: Information Extraction](#section1)\n",
    "* [Section 2: Chunking](#section2)\n",
    "    - [Section 2.1 Noun Phrase Chunking](#section2_1)\n",
    "    - [Section 2.3 Chunking with Regular Expressions](#section2_3)\n",
    "    - [Section 2.4 Exploring Corpora](#section2_4)\n",
    "    - [Section 2.5 Chinking](#section2_5)\n",
    "    - [Section 2.6 Representing Chunks: Tags vs Trees](#section2_6)\n",
    "* [Section 3: Developing Chunkers](#section3)\n",
    "    - [Section 3.1 Reading data](#section3_1)\n",
    "    - [Section 3.2 Simple Baselines](#section3_2)\n",
    "    - [Section 3.3 Training classifier-based chunkers](#section3_3)\n",
    "* [Section 4: Recursion in Linguistic Structure](#section4)\n",
    "    - [Section 4.1 Building Nested Structure](#section4_1)\n",
    "    - [Section 4.2 Trees](#section4_2)\n",
    "    - [Section 4.3 Tree traversal](#section4_3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Information Extraction\n",
    "We take an approach of converting the **unstructured data** of natural language sentences into the structured data (e.g. table). Then we reap the benefits of powerful query tools such as SQL. THis method of getting meaning from text is called **Information Extraction**.\n",
    "<a id=\"section11\"></a>\n",
    "### 1.1. Information Extraction Architecture\n",
    "<img src=\"figures/ie-architecture.png\" width=800>\n",
    "\n",
    "1. Raw text of the document is split into sentences using a sentence segmenter\n",
    "2. Each sentence is subdivided into words using a tokenizer\n",
    "3. Each sentence is tagged with part-of-speech tags\n",
    "4. **Named entity detection** - search for mentions of potentially interesting entities in each sentence\n",
    "5. **Relation detection** to search for likely relations between different entities in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## 2. Chunking\n",
    "[Back](#ch07)\n",
    "\n",
    "**Chunking** segments and labels multi-token sequences.\n",
    "<img src=\"figures/chunk-segmentation.png\" width=600>\n",
    "Segmentation and labelling at both the TOken and Chunk Levels.\n",
    "<a id=\"section2_1\"></a>\n",
    "### 2.1 Noun Phrase Chunking or NP-Chunking\n",
    "We search for chunks corresponding to individual noun phrases. \n",
    "\n",
    "In order to create an `NP-`chunker, we will first define a **chunk grammar**, consisting of rules that indiccate how sentences should be chunked. In this case, we will define a simple grammar with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (`DT`) followed by any number of adjectives (`JJ`) and then a noun (`NN`). \n",
    "\n",
    "Using this grammar, we create a chunk parser, and test it on our example sentence. The result is a tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/chunker_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2_3\"></a>\n",
    "### 2.3 Chunking with Regular Expressions\n",
    "[Back](#ch07)\n",
    "\n",
    "The rules that make up a chunk grammar use **tag patterns** to describe sequences of tagged words. A tag pattern is a sequence of part-of-speech tags delimited using angle brackets.\n",
    "\n",
    "To find the chunk structure for a given sentence, the `RegexpParser` chunker begins with a flat structure in which no tokens are chunked. The chunking rules are applied in turn, successively updating the chunk structure. Once all of the rules have been invoked, the resulting chunk structure is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN>}  # chunk determiner/possessive, adjectives and noun\n",
    "    {<NNP>+}                   # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Rapunzel/NNP)\n",
      "  let/VBD\n",
      "  down/RP\n",
      "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n"
     ]
    }
   ],
   "source": [
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tag pattern matches at overlapping locations, the leftmost match takes precedence. For example, if we apply a rule that matches two consecutive nouns to a text containing three consecutive nouns, then only the first two nouns will be chunked:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2_4\"></a>\n",
    "### 2.4  Exploring Text Corpora\n",
    "[Back](#ch07)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CHUNK combined/VBN to/TO achieve/VB)\n",
      "(CHUNK continue/VB to/TO place/VB)\n",
      "(CHUNK serve/VB to/TO protect/VB)\n",
      "(CHUNK wanted/VBD to/TO wait/VB)\n",
      "(CHUNK allowed/VBN to/TO place/VB)\n",
      "(CHUNK expected/VBN to/TO become/VB)\n",
      "(CHUNK expected/VBN to/TO approve/VB)\n",
      "(CHUNK expected/VBN to/TO make/VB)\n",
      "(CHUNK intends/VBZ to/TO make/VB)\n",
      "(CHUNK seek/VB to/TO set/VB)\n",
      "(CHUNK like/VB to/TO see/VB)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n",
    "brown = nltk.corpus.brown\n",
    "\n",
    "for i, sent in enumerate(brown.tagged_sents()):\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'CHUNK' and i < 100: print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunks(pattern):\n",
    "    c_type = pattern.split(':')[0]\n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    brown = nltk.corpus.brown\n",
    "\n",
    "    for i, sent in enumerate(brown.tagged_sents()):\n",
    "        tree = cp.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == c_type and i < 100: print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NOUNS Court/NN-TL Judge/NN-TL Durwood/NP Pye/NP)\n",
      "(NOUNS Mayor-nominate/NN-TL Ivan/NP Allen/NP Jr./NP)\n",
      "(NOUNS Georgia's/NP$ automobile/NN title/NN law/NN)\n",
      "(NOUNS State/NN-TL Welfare/NN-TL Department's/NN$-TL handling/NN)\n",
      "(NOUNS Fulton/NP-TL Tax/NN-TL Commissioner's/NN$-TL Office/NN-TL)\n",
      "(NOUNS Mayor/NN-TL William/NP B./NP Hartsfield/NP)\n",
      "(NOUNS Mrs./NP J./NP M./NP Cheshire/NP)\n",
      "(NOUNS E./NP Pelham/NP Rd./NN-TL Aj/NN)\n",
      "(NOUNS\n",
      "  State/NN-TL\n",
      "  Party/NN-TL\n",
      "  Chairman/NN-TL\n",
      "  James/NP\n",
      "  W./NP\n",
      "  Dorsey/NP)\n",
      "(NOUNS Texas/NP Sen./NN-TL John/NP Tower/NP)\n",
      "(NOUNS Lt./NN-TL Gov./NN-TL Garland/NP Byrd's/NP$ campaign/NN)\n",
      "(NOUNS Schley/NP County/NN-TL Rep./NN-TL B./NP D./NP Pelham/NP)\n",
      "(NOUNS Colquitt/NP-TL Policeman/NN-TL Tom/NP Williams/NP)\n"
     ]
    }
   ],
   "source": [
    "find_chunks('NOUNS: {<N.*>{4,}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2_5\"></a>\n",
    "### 2.5 Chinking\n",
    "[Back](#ch07)\n",
    "\n",
    "We define a **chink** to be a sequence of tokens that is not included in a chunk. Here `barked/VBD at/IN` is a chink\n",
    "```\n",
    "[ the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]\n",
    "```\n",
    "Chinking is the process of removing a sequence of tokens from a chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP:\n",
    "        {<.*>+}          # Chunk everything\n",
    "        }<VBD|IN>+{     # Chink sequences of VBD and IN\n",
    "\"\"\"\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2_6\"></a>\n",
    "### 2.6 Representing Chunks: Tags vs Trees\n",
    "[Back](#ch07)\n",
    "\n",
    "Chunk structures can be represented using either tags or trees. The most widspread file representation uses **IOB tags**. In this shceme, each token is tagged with one of three special chunk tags, `I`(inside), `O`(outside) or `B`(begin). A token is tagged as `B` if it marks the beginning of a chunk.\n",
    "<img src=\"figures/chunk-tagrep.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 3. Developing and Evaluating Chunkers\n",
    "[Back](#ch07)\n",
    "### 3.1 Reading IOB Format and the CoNLL 2000 Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "position NN I-NP\n",
    "of IN B-PP\n",
    "vice NN B-NP\n",
    "chairman NN I-NP\n",
    "of IN B-PP\n",
    "Carlyle NNP B-NP\n",
    "Group NNP I-NP\n",
    ", , O\n",
    "a DT B-NP\n",
    "merchant NN I-NP\n",
    "banking NN I-NP\n",
    "concern NN I-NP\n",
    ". . O\n",
    "'''\n",
    "nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/chunk_tree.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "print(conll2000.chunked_sents('train.txt')[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2_3\"></a>\n",
    "### 3.2 Simple Evaluation and Baselines\n",
    "[Back](#ch07)\n",
    "\n",
    "Now that we can access a chunked corpus, we can evaluate chunkers. We start off by establishing a baseline for the trivial chunk parser `cp` that creates no chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive regular expression chunker that looks for tags beginning with letters that are characteristic of noun phrase tags (e.g. `CD`, `DT`, and `JJ`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach achieves decent results, but we can improve on it by adopting a data-driven approach, where we use the training corpus to find the chunk tag (`I`, `O` or `B`) that is most likely for each part-of-speech tag. In other words, we can build a chunker using a *unigram tagger*. But rather than trying to determine the correct part-of-speech tag for each word, we are trying to determine the correct chunk tag, given each word's part-of-speech tag.\n",
    "\n",
    "We define the `UnigramChunker` class, which uses a unigram tagger to label sentences with chunk tags. Most of the code in this class is simply used to convert back and forth between the chunk tree representation used by NLTK's `ChunkParserI` interface, and the IOB representation used by the embedded tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\n",
    "                     for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
    "                    in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print(unigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunker does reasonably well, achieving an overall f-measure score of 83%. Let's take a look at what it's learned, by using its unigram tagger to assign a tag to each of the part-of-speech tags that appear in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'), (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'), ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'), ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'), ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'), ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'), ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'), ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'), ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n"
     ]
    }
   ],
   "source": [
    "postags = sorted(set(pos for sent in train_sents\n",
    "                    for (word, pos) in sent.leaves()))\n",
    "print(unigram_chunker.tagger.tag(postags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram Chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\n",
    "                     for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
    "                    in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.5%%\n"
     ]
    }
   ],
   "source": [
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3_3\"></a>\n",
    "### 3.3 Training Classifier-Based Chunkers\n",
    "[Back](#ch07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.path.abspath(os.path.curdir)\n",
    "os.environ[\"MEGAM\"] = os.path.join(cwd, 'MEGAM/megam-64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append((featureset, tag))\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.MaxentClassifier.train(\n",
    "            train_set, algorithm='megam', trace=0)\n",
    "    \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w, t), c) for (w, t, c) in\n",
    "                        nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w, t, c) for ((w, t), c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a feature for the previous part-of-speech tag. Adding this feature allows the classifier to model interactions between adjacent tags, and results in a chunker that is closely related to the bigram chunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    return {'pos': pos, 'prevpos': prevpos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.6%%\n",
      "    Precision:     81.9%%\n",
      "    Recall:        87.1%%\n",
      "    F-Measure:     84.4%%\n"
     ]
    }
   ],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try adding a feature for the current word, since we hypothesized that word content should be useful for chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    return {'pos': pos, 'word': word, 'prevpos': prevpos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.1%%\n",
      "    Precision:     82.9%%\n",
      "    Recall:        87.9%%\n",
      "    F-Measure:     85.3%%\n"
     ]
    }
   ],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can try extending the feature extractor with a variety of additional features, such as lookahead features, paired features, and complex contextual feature. This last feature, called `tags-since-dt` creates a string describing the set of all part-of-speech tags that have been encountered since the most recent determiner, or since the beginning of the sentence if there is no determiner before index `i`.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "    if i == len(sentence)-1:\n",
    "        nextword, nextpos = \"<END>\", \"<END>\"\n",
    "    else:\n",
    "        nextword, nextpos = sentence[i+1]\n",
    "    return {\"pos\": pos,\n",
    "             \"word\": word,\n",
    "             \"prevpos\": prevpos,\n",
    "             \"nextpos\": nextpos, \n",
    "             \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),  \n",
    "             \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\n",
    "             \"tags-since-dt\": tags_since_dt(sentence, i)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_since_dt(sentence, i):\n",
    "    tags = set()\n",
    "    for word, pos in sentence[:i]:\n",
    "        if pos == 'DT':\n",
    "            tags = set()\n",
    "        else:\n",
    "            tags.add(pos)\n",
    "    return '+'.join(sorted(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  96.0%%\n",
      "    Precision:     88.8%%\n",
      "    Recall:        91.1%%\n",
      "    F-Measure:     89.9%%\n"
     ]
    }
   ],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## 4. Recursion in Linguistic Structure\n",
    "[Back](#ch07)\n",
    "<a id=\"section4_1\"></a>\n",
    "### 4.1 Building Nested Structure with Cascaded Chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "   NP: {<DT|JJ|NN.*>+}          # chunk sequences of DT, JJ, NN\n",
    "   PP: {<IN><NP>}               # chunk prepositions followed by NP\n",
    "   VP: {<VB.*><NP|PP|CLAUSE>+$} # chunk verbs and their arguments\n",
    "   CLAUSE: {<NP><VP>}           # chunk NP, VP\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
    "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence with deeper nesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\n",
    "     (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n",
    "     (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (CLAUSE\n",
      "    (NP Mary/NN)\n",
      "    (VP\n",
      "      saw/VBD\n",
      "      (CLAUSE\n",
      "        (NP the/DT cat/NN)\n",
      "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar, loop=2)\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4_2\"></a>\n",
    "### 4.2 Trees\n",
    "[Back](#ch07)\n",
    "\n",
    "In NLTK, we create a tree by giving a node label and a list of children:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP Alice)\n",
      "(NP the rabbit)\n"
     ]
    }
   ],
   "source": [
    "tree1 = nltk.Tree('NP', ['Alice'])\n",
    "print(tree1)\n",
    "tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
    "print(tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can coorporate these into successively larger trees as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP chased (NP the rabbit)))\n"
     ]
    }
   ],
   "source": [
    "tree3 = nltk.Tree('VP', ['chased', tree2])\n",
    "tree4 = nltk.Tree('S', [tree1, tree3])\n",
    "print(tree4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VP'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree4[1].label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice', 'chased', 'the', 'rabbit']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree4.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rabbit'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree4[1][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4_3\"></a>\n",
    "### 4.3 Tree Traversal\n",
    "[Back](#ch07)\n",
    "\n",
    "It is standard to use a recursive function to traverse a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(t):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        print(t, end=\" \")\n",
    "    else:\n",
    "        # Now we know that t.node is defined\n",
    "        print('(', t.label(), end=\" \")\n",
    "        for child in t:\n",
    "            traverse(child)\n",
    "        print(')', end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) ) "
     ]
    }
   ],
   "source": [
    "t = nltk.Tree('S', [nltk.Tree('NP', ['Alice']), nltk.Tree('VP', ['chased', nltk.Tree('NP', ['the rabbit'])])])\n",
    "traverse(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "## 5 Named Entity Recognition\n",
    "This can be broken down into two sub-tasks:\n",
    "- Identifying the boundaries of the NE\n",
    "- Identifying its type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  plant/NN\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  *T*-1/-NONE-\n",
      "  is/VBZ\n",
      "  owned/VBN\n",
      "  *-4/-NONE-\n",
      "  by/IN\n",
      "  (NE Hollingsworth/NNP)\n",
      "  &/CC\n",
      "  (NE Vose/NNP Co./NNP)\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  under/IN\n",
      "  contract/NN\n",
      "  *ICH*-2/-NONE-\n",
      "  with/IN\n",
      "  (NE Lorillard/NN)\n",
      "  */-NONE-\n",
      "  to/TO\n",
      "  make/VB\n",
      "  the/DT\n",
      "  cigarette/NN\n",
      "  filters/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sent = nltk.corpus.treebank.tagged_sents()[20]\n",
    "print(nltk.ne_chunk(sent, binary=True)) # Named entities are taked as NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  plant/NN\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  *T*-1/-NONE-\n",
      "  is/VBZ\n",
      "  owned/VBN\n",
      "  *-4/-NONE-\n",
      "  by/IN\n",
      "  (ORGANIZATION Hollingsworth/NNP)\n",
      "  &/CC\n",
      "  (PERSON Vose/NNP Co./NNP)\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  under/IN\n",
      "  contract/NN\n",
      "  *ICH*-2/-NONE-\n",
      "  with/IN\n",
      "  (PERSON Lorillard/NN)\n",
      "  */-NONE-\n",
      "  to/TO\n",
      "  make/VB\n",
      "  the/DT\n",
      "  cigarette/NN\n",
      "  filters/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(sent)) # Classifier adds category labels such as PERSON, ORGANISATION and GPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section6\"></a>\n",
    "## 6. Relation Extraction\n",
    "[Back](#ch07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern=IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `conll2002` Dutch corpus contains not just named entity annotation but also part-of-speech tags. This allows us to devise patterns that are sensitive to these tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "vnv = \"\"\"\n",
    "(\n",
    "is/V|       # 3rd sing present and\n",
    "was/V|      # pastforms of the verb zijn ('be')\n",
    "werd/V|     # and also present\n",
    "wordt/V     # past of worden ('become')\n",
    ")\n",
    ".*          # followed by anything\n",
    "van/Prep    # followed by van ('of')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAN(\"cornet_d'elzius\", 'buitenlandse_handel')\n",
      "VAN('johan_rottiers', 'kardinaal_van_roey_instituut')\n",
      "VAN('annie_lennox', 'eurythmics')\n"
     ]
    }
   ],
   "source": [
    "VAN = re.compile(vnv, re.VERBOSE)\n",
    "for doc in conll2002.chunked_sents('ned.train'):\n",
    "    for rel in nltk.sem.extract_rels('PER', 'ORG', doc, corpus='conll2002', pattern=VAN):\n",
    "        print(nltk.sem.clause(rel, relsym=\"VAN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show actual words between the two NEs and their left and right context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...'')[PER: \"Cornet/V d'Elzius/N\"] 'is/V op/Prep dit/Pron ogenblik/N kabinetsadviseur/N van/Prep staatssecretaris/N voor/Prep' [ORG: 'Buitenlandse/N Handel/N'](''...\n",
      "...'')[PER: 'Johan/N Rottiers/N'] 'is/V informaticaco√∂rdinator/N van/Prep het/Art' [ORG: 'Kardinaal/N Van/N Roey/N Instituut/N']('in/Prep'...\n",
      "...'Door/Prep rugproblemen/N van/Prep zangeres/N')[PER: 'Annie/N Lennox/N'] 'wordt/V het/Art concert/N van/Prep' [ORG: 'Eurythmics/N']('vandaag/Adv in/Prep'...\n"
     ]
    }
   ],
   "source": [
    "VAN = re.compile(vnv, re.VERBOSE)\n",
    "for doc in conll2002.chunked_sents('ned.train'):\n",
    "    for rel in nltk.sem.extract_rels('PER', 'ORG', doc, corpus='conll2002', pattern=VAN):\n",
    "        print(nltk.rtuple(rel, lcon=True, rcon=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python37364bitvenvvirtualenvc41ffb309c4a48a8a3d1bf11d2652770"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
